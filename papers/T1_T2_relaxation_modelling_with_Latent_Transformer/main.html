<!DOCTYPE html><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer</title>
<!--Generated on Thu Sep 28 16:08:25 2023 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="../ar5iv.0.7.7.min.css" type="text/css">
<link rel="stylesheet" href="../ar5iv-site.0.2.1.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Quantitative MRI Deep learning MRI reconstruction">

<script>
    function detectColorScheme(){
        var theme="light";
        var current_theme = localStorage.getItem("ar5iv_theme");
        if(current_theme){
            if(current_theme == "dark"){
                theme = "dark";
            } }
        else if(!window.matchMedia) { return false; }
        else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
            theme = "dark"; }
        if (theme=="dark") {
            document.documentElement.setAttribute("data-theme", "dark");
        } else {
            document.documentElement.setAttribute("data-theme", "light"); } }

    detectColorScheme();

    function toggleColorScheme(){
        var current_theme = localStorage.getItem("ar5iv_theme");
        if (current_theme) {
            if (current_theme == "light") {
                localStorage.setItem("ar5iv_theme", "dark"); }
            else {
                localStorage.setItem("ar5iv_theme", "light"); } }
        else {
            localStorage.setItem("ar5iv_theme", "dark"); }
        detectColorScheme(); }
</script>

</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fanwen Wang
</span><span class="ltx_author_notes"><span>Equal contribution<br>
Imperial College London, Exhibition Road, London SW7 2AZ, UK<br>
Royal Brompton and Harefield hospital, Sydney Street, London SW3 6NP, UK</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Tänzer
</span><span class="ltx_author_notes"><span>Equal contribution<br>
Imperial College London, Exhibition Road, London SW7 2AZ, UK<br>
Royal Brompton and Harefield hospital, Sydney Street, London SW3 6NP, UK</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mengyun Qiao
</span><span class="ltx_author_notes"><span>
Imperial College London, Exhibition Road, London SW7 2AZ, UK</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenjia Bai
</span><span class="ltx_author_notes"><span>
Imperial College London, Exhibition Road, London SW7 2AZ, UK</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Rueckert
</span><span class="ltx_author_notes"><span>
Imperial College London, Exhibition Road, London SW7 2AZ, UK<br>
Technische Universität München (TUM), Arcisstraße 21, 80333 München, Germany</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guang Yang
</span><span class="ltx_author_notes"><span>
Imperial College London, Exhibition Road, London SW7 2AZ, UK<br>
Royal Brompton and Harefield hospital, Sydney Street, London SW3 6NP, UK</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sonia Nielles-Vallespin
</span><span class="ltx_author_notes"><span>
Imperial College London, Exhibition Road, London SW7 2AZ, UK<br>
Royal Brompton and Harefield hospital, Sydney Street, London SW3 6NP, UK</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>

<p class="ltx_p">Quantitative cardiac magnetic resonance T1 and T2 mapping enable myocardial tissue characterisation but the lengthy scan times restrict their widespread clinical application. We propose a deep learning method that incorporates a time dependency <span class="ltx_text ltx_font_italic">Latent Transformer</span> module to model relationships between parameterised time frames for improved reconstruction from undersampled data. The module, implemented as a multi-resolution sequence-to-sequence transformer, is integrated into an encoder-decoder architecture to leverage the inherent temporal correlations in relaxation processes. The presented results for accelerated T1 and T2 mapping show the model recovers maps with higher fidelity by explicit incorporation of time dynamics. This work demonstrates the importance of temporal modelling for artifact-free reconstruction in quantitative MRI.</p>

</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Quantitative MRI Deep learning MRI reconstruction
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Magnetic resonance imaging (MRI) is a crucial non-invasive tool for assessing tissue properties and functions, with applications across medical disciplines. In cardiac imaging, quantitative T1 and T2 mapping provide insights into myocardial composition and structure, enabling characterisation of cardiomyocytes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="Myocardial T1 mapping: application to patients with acute and chronic myocardial infarction" class="ltx_ref">6</a>, <a href="#bib.bib13" title="Identification and assessment of Anderson-Fabry disease by cardiovascular magnetic resonance noncontrast myocardial T1 mapping" class="ltx_ref">10</a>, <a href="#bib.bib14" title="Myocardial T1 mapping and extracellular volume quantification: a Society for Cardiovascular Magnetic Resonance (SCMR) and CMR Working Group of the European Society of Cardiology consensus statement" class="ltx_ref">8</a>]</cite>. However, long scan times limit the clinical utility of cardiac T1 and T2 mapping. Although undersampled acquisitions offer a means to accelerate scans, they often lead to artifacts and errors, not only in the reconstructed images but also to a greater extent in the computed T1/T2 maps.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Recent deep learning approaches have shown promise for reconstructing high-quality maps from highly accelerated scans. Encoder-decoder models like AUTOMAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Image reconstruction by domain-transform manifold learning" class="ltx_ref">16</a>]</cite> leveraged deep convolutional neural networks to learn efficient representations directly from undersampled k-space and mapping targets. Schlemper et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="A deep cascade of convolutional neural networks for dynamic MR image reconstruction" class="ltx_ref">11</a>]</cite> explored the ability of cascaded CNN to learn the spatial-temporal correlations from multi-coil undersampled cardiac cine MRI. Qin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="Complementary time-frequency domain networks for dynamic parallel MR image reconstruction" class="ltx_ref">9</a>]</cite> exploited the spatiotemporal correlations using recurrent network for dynamic multi-coil cardiac cine data. Lyu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="Region-focused multi-view transformer-based generative adversarial network for cardiac cine mri reconstruction" class="ltx_ref">4</a>]</cite> divided temporal cine MRI data into several views and used a video-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="Multiview transformers for video recognition" class="ltx_ref">14</a>]</cite> model to capture spatial and temporal relationship. However, most existing methods disregard dependencies between parameterised time frames. As relaxation processes induce temporal correlations, explicitly modelling time structure is essential for accurate reconstruction.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">We introduce an innovative deep reconstruction model that introduces a temporal dependency module to effectively capture inter-frame relationships within encoded sequences. The module is seamlessly integrated into an encoder-decoder architecture by modifying the latent vectors in the skip connections of the encoder-decoder model to better exploit the temporal correlation. By incorporating temporal dynamics, the proposed model aims to significantly enhance reconstructions derived from accelerated cardiac T1 and T2 mapping scans, regardless of the number of time points or mapping methodologies employed. This could facilitate accurate analyses of myocardial tissue properties from faster, patient-friendly scans. We present preliminary validation of our technique for accelerated cardiac T1 and T2 mapping against state-of-the-art methods and gold-standard fully-sampled acquisitions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Acquisition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">We used both single and multi-coil T1 and T2 mapping data from the MICCAI 2023 CMRxRecon training dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="CMRxRecon: an open cardiac mri dataset for the competition of accelerated image reconstruction" class="ltx_ref">12</a>]</cite>. The T1 mapping data was acquired using a Modified Look-Locker Inversion recovery (MOLLI) sequence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="Modified Look-Locker inversion recovery (MOLLI) for high-resolution T1 mapping of the heart" class="ltx_ref">7</a>]</cite> with nine frames of variable T1 weighting in short-axis view at end-diastole. For each subject, between five and seven slices were collected with a slice thickness of 5.0 mm. The matrix size of each T1-weighted frame was <math id="S2.SS1.p1.m1" class="ltx_Math" alttext="144\times 512" display="inline"><mrow><mn>144</mn><mo>×</mo><mn>512</mn></mrow></math> with an in-plane spatial resolution of 1.4 x 1.4 mm<math id="S2.SS1.p1.m2" class="ltx_Math" alttext="{}^{2}" display="inline"><msup><mi></mi><mn>2</mn></msup></math>. For the multi-coil data, the coils were compressed to 10 virtual coils. The T2 mapping data was acquired using a T2-prepared (T2prep) FLASH sequence with three T2 weightings and with geometrical parameters identical to the T1 mapping acquisition.
</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data processing</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">Both single and multi-coil T1 and T2 mapping data from the 120 healthy subjects in the training dataset were randomly split into 80% for training, 10% for validation and 10% for testing. We pre-processed the provided k-space data by scaling it to a range where the model could perform optimally. Specifically, we multiplied all k-space data by a fixed value of <math id="S2.SS2.p1.m1" class="ltx_Math" alttext="10^{2}" display="inline"><msup><mn>10</mn><mn>2</mn></msup></math> to bring the magnitude of the images values approximately into the [0, 1] range. This transformation is reversed before computing the mappings.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">During training, the data was also augmented using a random undersampling mask for every subject. The random mask was generated by selecting every <math id="S2.SS2.p2.m1" class="ltx_Math" alttext="k^{\textrm{th}}" display="inline"><msup><mi>k</mi><mtext>th</mtext></msup></math> line starting from line <math id="S2.SS2.p2.m2" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>, where <math id="S2.SS2.p2.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is the acceleration factor and <math id="S2.SS2.p2.m4" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> is a randomly sampled integer between 0 and <math id="S2.SS2.p2.m5" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>. As in the original acquisition, the random mask preserved the central 24 lines of k-space. This allowed the model to exhibit greater adaptability to minor variations in the acquisition protocol, thereby providing a valuable and realistic data augmentation strategy.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Latent transformer</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p">The proposed model, the Latent Transformer (LT), employs an encoder-decoder architecture with shared encoding-decoding blocks across all time-frames (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.3.2 Single-coil ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Embedded within each skip connection between an encoding layer and the corresponding decoding layer is an LT block, which enables modelling of dependencies across frames before passing signals to the decoder layers (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.3.2 Single-coil ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, E). Specifically, there is a unique LT block for each layer of the main encoder-decoder network. Each LT block utilises multi-layer and multi-head self-attention (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.3.2 Single-coil ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, C and D) to compute the updated latent code as a weighted linear combination of itself and the latent codes of the other time-frames in a pixel-wise manner (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.3.2 Single-coil ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, A and B). The LT blocks are key to exploit temporal correlations within the data to aid reconstruction performance, as they allow information from each frame to affect the reconstruction of all other frames.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Single-coil</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p">The proposed model architecture consists of two complementary components as illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3.2 Single-coil ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div id="S2.I1.i1.p1" class="ltx_para">
<p class="ltx_p">An encoder-decoder network that serves as the main artifact removal model. In our experiments, we utilise a U-Net architecture as a strong baseline model. The goal of this encoder-decoder network is to remove large-scale artifacts from the input MRI frames.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div id="S2.I1.i2.p1" class="ltx_para">
<p class="ltx_p">The latent transformer model to explicitly capture inter-frame dependencies.</p>
</div>
</li>
</ul>
<p class="ltx_p">Finally, the predictions from the main encoder-decoder model and the LT model are combined to produce the final reconstructed output frames. By fusing the outputs this way, the model leverages both general artifact removal capabilities and inter-frame dependencies for enhanced MRI reconstruction.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="x15.png" id="S2.F1.g1" class="ltx_graphics ltx_centering" width="676" height="425" alt="Diagram representing the architecture for the single-coil (E) and multi-coil (F) tasks. The figure also shows how the latent transformer is implemented using a pixel-wise self-attention mechanism (A, B) in a multilayer and multi-head fashion (D). The figure shows a case where three time-frames are available, but the method extends seamlessly to any number of time-frames with no modification.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram representing the architecture for the single-coil (E) and multi-coil (F) tasks. The figure also shows how the latent transformer is implemented using a pixel-wise self-attention mechanism (A, B) in a multilayer and multi-head fashion (D). The figure shows a case where three time-frames are available, but the method extends seamlessly to any number of time-frames with no modification.</figcaption>
</figure>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>Multi-coil model</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p">Similar to the single-coil model, the multi-coil artifact-removal architecture consists of a main encoder-decoder network and LT model tailored for multi-coil data. To reduce computational complexity, the LT is applied on the coil-combined complex image rather than each coil individually. Coil sensitivity maps (CSMs) <math id="S2.SS3.SSS3.p1.m1" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> are extracted using an iterative approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="A fast optimal method for coil sensitivity estimation and adaptive coil combination for complex images" class="ltx_ref">3</a>]</cite> with smoothing based on the central 24 lines of undersampled k-space <math id="S2.SS3.SSS3.p1.m2" class="ltx_Math" alttext="x\in{\mathbb{C}^{W\times M}}" display="inline"><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℂ</mi><mrow><mi>W</mi><mo>×</mo><mi>M</mi></mrow></msup></mrow></math> among all <math id="S2.SS3.SSS3.p1.m3" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> coils. The undersampled multi-coil data is multiplied by the conjugate CSMs to maintain complex information as <math id="S2.SS3.SSS3.p1.m4" class="ltx_Math" alttext="\hat{x}\in{\mathbb{C}^{W}}" display="inline"><mrow><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mo>∈</mo><msup><mi>ℂ</mi><mi>W</mi></msup></mrow></math> before input to the model.</p>
</div>
<div id="S2.SS3.SSS3.p2" class="ltx_para">
<p class="ltx_p">The multi-coil reconstruction can be formulated as:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1" class="ltx_Math" alttext="\hat{x}_{rec}=\mathop{\arg\min}\limits_{x}(\left\|{Ex-y}\right\|_{2}^{2}+%
\lambda\left\|{\hat{x}-S(\hat{x};\theta)}\right\|_{2}^{2})" display="block"><mrow><msub><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>c</mi></mrow></msub><mo>=</mo><mrow><munder><mrow><mi>arg</mi><mo movablelimits="false">⁡</mo><mi>min</mi></mrow><mi>x</mi></munder><mrow><mo stretchy="false">(</mo><mrow><msubsup><mrow><mo>∥</mo><mrow><mrow><mi>E</mi><mo>⁢</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mi>λ</mi><mo>⁢</mo><msubsup><mrow><mo>∥</mo><mrow><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mo>-</mo><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</table>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1" class="ltx_Math" alttext="E=M\cdot F\cdot\hat{C}" display="block"><mrow><mi>E</mi><mo>=</mo><mrow><mi>M</mi><mo>⋅</mo><mi>F</mi><mo>⋅</mo><mover accent="true"><mi>C</mi><mo stretchy="false">^</mo></mover></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr>
</table>
<p class="ltx_p">where <math id="S2.SS3.SSS3.p2.m1" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> is the multi-coil complex image and <math id="S2.SS3.SSS3.p2.m2" class="ltx_Math" alttext="y\in{\mathbb{C}^{W\times M}}" display="inline"><mrow><mi>y</mi><mo>∈</mo><msup><mi>ℂ</mi><mrow><mi>W</mi><mo>×</mo><mi>M</mi></mrow></msup></mrow></math> is the acquired multi-coil k-space data. <math id="S2.SS3.SSS3.p2.m3" class="ltx_Math" alttext="E" display="inline"><mi>E</mi></math> represents the operator combining the undersampling mask <math id="S2.SS3.SSS3.p2.m4" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>, Fourier transform <math id="S2.SS3.SSS3.p2.m5" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math>, and updated sensitivity maps <math id="S2.SS3.SSS3.p2.m6" class="ltx_Math" alttext="\hat{C}" display="inline"><mover accent="true"><mi>C</mi><mo stretchy="false">^</mo></mover></math>. <math id="S2.SS3.SSS3.p2.m7" class="ltx_Math" alttext="S(\hat{x};\theta)" display="inline"><mrow><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></math> denotes the single-coil based deep neural network with parameters <math id="S2.SS3.SSS3.p2.m8" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math>.
We separate the optimisation into conjugate gradient SENSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="CG-SENSE revisited: Results from the first ISMRM reproducibility challenge" class="ltx_ref">5</a>]</cite> reconstruction and neural network-based reconstruction, iteratively updating <math id="S2.SS3.SSS3.p2.m9" class="ltx_Math" alttext="\hat{x}" display="inline"><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover></math>.</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1" class="ltx_Math" alttext="\hat{x}_{rec}=(E^{H}E+\lambda I)^{-1}(E^{H}y+\lambda S(x;\theta))" display="block"><mrow><msub><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>c</mi></mrow></msub><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>E</mi><mi>H</mi></msup><mo>⁢</mo><mi>E</mi></mrow><mo>+</mo><mrow><mi>λ</mi><mo>⁢</mo><mi>I</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>E</mi><mi>H</mi></msup><mo>⁢</mo><mi>y</mi></mrow><mo>+</mo><mrow><mi>λ</mi><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr>
</table>
<p class="ltx_p">where <math id="S2.SS3.SSS3.p2.m10" class="ltx_Math" alttext="\hat{x}_{rec}" display="inline"><msub><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>c</mi></mrow></msub></math> is calculated with fixed <math id="S2.SS3.SSS3.p2.m11" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math> parameters in the network.</p>
</div>
<div id="S2.SS3.SSS3.p3" class="ltx_para">
<p class="ltx_p">An additional CSM update module is integrated to improve the original <math id="S2.SS3.SSS3.p3.m1" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> under the supervision of <math id="S2.SS3.SSS3.p3.m2" class="ltx_Math" alttext="\hat{x}_{rec}" display="inline"><msub><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>c</mi></mrow></msub></math> for a better SENSE reconstruction. The CSM <math id="S2.SS3.SSS3.p3.m3" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> initialised by iterative method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="A fast optimal method for coil sensitivity estimation and adaptive coil combination for complex images" class="ltx_ref">3</a>]</cite> works as a warm start:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1" class="ltx_Math" alttext="\hat{C}=N(C;\beta)" display="block"><mrow><mover accent="true"><mi>C</mi><mo stretchy="false">^</mo></mover><mo>=</mo><mrow><mi>N</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>;</mo><mi>β</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr>
</table>
<p class="ltx_p"><math id="S2.SS3.SSS3.p3.m4" class="ltx_Math" alttext="N(C;\beta)" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>C</mi><mo>;</mo><mi>β</mi><mo stretchy="false">)</mo></mrow></mrow></math> is the network to update the CSM with parameters of <math id="S2.SS3.SSS3.p3.m5" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math>. It consisted of four single-scale convolutional layers with kernel size of 3<math id="S2.SS3.SSS3.p3.m6" class="ltx_Math" alttext="\times" display="inline"><mo>×</mo></math>3 followed by ReLU and a fifth layer with only 2D convolution.
</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Assessments</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p">The results reported in this work were computed by comparing the model outputs with fully-sampled reconstructions on a fixed test set. We reported quantitative metrics including root-mean-square-error (RMSE), normalised mean-square-error (NMSE), peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM). To assess the impact on the downstream mapping task, these metrics were also calculated for the estimated T1 and T2 parameter maps.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p class="ltx_p">To match the evaluation protocol used in the CMRxRecon challenge, the reconstructed images and parameter maps were cropped to a region of interest before metric computation. The quantitative assessment was specifically focused on the most clinically relevant region by retaining only the central 50% of rows and central 33% of columns.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Single-coil reconstruction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">To evaluate the proposed model, we conducted experiments for both the T1 and T2 mapping tasks using acceleration factors 4, 8, and 10. In this section, we compared two model configurations:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p">U-Net: A baseline U-Net architecture that serves as a standard encoder-decoder network.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p">U-Net + Latent Transformer: The proposed model combining the baseline encoder-decoder model with the latent-transformer module to exploit inter-frame dependencies.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Single-coil reconstruction ‣ 3 Results ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarises the results across both tasks for the three considered acceleration factors. Results demonstrate the ability of the latent transformer to effectively model the temporal correlations and improve the reconstruction.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Single-coil reconstruction ‣ 3 Results ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> also qualitatively compares the reconstruction produced using Zero-filling, a U-Net model and the proposed U-Net + LT model.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:435.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.6pt,20.8pt) scale(0.912898140246473,0.912898140246473) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">PSNR ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">SSIM ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">NMSE ↓</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">RMSE ↓</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">U-Net T1 4×</th>
<td class="ltx_td ltx_align_right ltx_border_t">30.160</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.811</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.028</td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">4.35E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 8×</th>
<td class="ltx_td ltx_align_right">28.158</td>
<td class="ltx_td ltx_align_right">0.766</td>
<td class="ltx_td ltx_align_right">0.044</td>
<td class="ltx_td ltx_align_right">5.58E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 10×</th>
<td class="ltx_td ltx_align_right">27.661</td>
<td class="ltx_td ltx_align_right">0.762</td>
<td class="ltx_td ltx_align_right">0.048</td>
<td class="ltx_td ltx_align_right">5.91E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 4×</th>
<td class="ltx_td ltx_align_right">28.940</td>
<td class="ltx_td ltx_align_right">0.827</td>
<td class="ltx_td ltx_align_right">0.023</td>
<td class="ltx_td ltx_align_right">2.81E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 8×</th>
<td class="ltx_td ltx_align_right">27.350</td>
<td class="ltx_td ltx_align_right">0.802</td>
<td class="ltx_td ltx_align_right">0.032</td>
<td class="ltx_td ltx_align_right">3.44E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 10×</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">27.246</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.808</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.032</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">3.50E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">U-Net T1 4× + LT</th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">30.184</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">0.813</span></td>
<td class="ltx_td ltx_align_right ltx_border_t">0.028</td>
<td class="ltx_td ltx_align_right ltx_border_t">4.36E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 8× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">28.431</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.774</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.040</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">5.39E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 10× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">27.933</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.769</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.045</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">5.71E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 4× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">29.067</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.831</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.022</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">2.76E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 8× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">27.469</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.806</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.031</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">3.39E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 10× + LT</th>
<td class="ltx_td ltx_align_right">27.210</td>
<td class="ltx_td ltx_align_right">0.807</td>
<td class="ltx_td ltx_align_right">0.033</td>
<td class="ltx_td ltx_align_right">3.51E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map PSNR ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map SSIM ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map NMSE ↓</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map RMSE ↓</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">U-Net T1 4×</th>
<td class="ltx_td ltx_align_right ltx_border_t">19.709</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.600</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.024</td>
<td class="ltx_td ltx_align_right ltx_border_t">134.219</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 8×</th>
<td class="ltx_td ltx_align_right">18.593</td>
<td class="ltx_td ltx_align_right">0.534</td>
<td class="ltx_td ltx_align_right">0.031</td>
<td class="ltx_td ltx_align_right">152.737</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 10×</th>
<td class="ltx_td ltx_align_right">18.548</td>
<td class="ltx_td ltx_align_right">0.524</td>
<td class="ltx_td ltx_align_right">0.031</td>
<td class="ltx_td ltx_align_right">153.266</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 4×</th>
<td class="ltx_td ltx_align_right">11.884</td>
<td class="ltx_td ltx_align_right">0.433</td>
<td class="ltx_td ltx_align_right">0.505</td>
<td class="ltx_td ltx_align_right">64.175</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 8×</th>
<td class="ltx_td ltx_align_right">11.838</td>
<td class="ltx_td ltx_align_right">0.406</td>
<td class="ltx_td ltx_align_right">0.511</td>
<td class="ltx_td ltx_align_right">64.483</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 10×</th>
<td class="ltx_td ltx_align_right">11.860</td>
<td class="ltx_td ltx_align_right">0.410</td>
<td class="ltx_td ltx_align_right">0.508</td>
<td class="ltx_td ltx_align_right">64.350</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">U-Net T1 4× + LT</th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">19.834</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">0.607</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">0.023</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">132.514</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 8× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">18.778</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.538</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.029</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">149.393</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T1 10× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">18.592</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.526</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.030</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">152.493</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 4× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">11.910</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.439</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.502</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">63.977</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">U-Net T2 8× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">11.898</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.413</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.504</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">64.056</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">U-Net T2 10× + LT</th>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">11.903</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">0.414</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">0.503</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">64.017</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Single-coil model results for both the reconstructed MR acquisitions and the computed T1 and T2 maps. The table compares a U-Net based artifact-removal process with a pipeline that used a Latent Transformer-enhanced U-Net at its core. Our report underlined the best model for a given mapping task and acceleration factor.</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="x16.png" id="S3.F2.g1" class="ltx_graphics ltx_centering" width="677" height="411" alt="Qualitative comparison between zero-filling reconstruction, U-Net-based reconstruction and the proposed U-Net + LT model. The figure shows both the reconstructed images and the T1 mapping associated with the shown slice.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Qualitative comparison between zero-filling reconstruction, U-Net-based reconstruction and the proposed U-Net + LT model. The figure shows both the reconstructed images and the T1 mapping associated with the shown slice.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-coil reconstruction</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">To evaluate the proposed model for multi-coil MRI reconstruction, we compared two model configurations built upon MoDL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="MoDL: Model-Based Deep Learning Architecture for Inverse Problems" class="ltx_ref">1</a>]</cite> framework:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div id="S3.I2.i1.p1" class="ltx_para">
<p class="ltx_p">MoDL: The baseline MoDL model using fixed coil sensitivity maps and a standard single-scale network architecture. This serves as the standard MoDL implementation.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div id="S3.I2.i2.p1" class="ltx_para">
<p class="ltx_p">MoDL + Proposed Model: An enhanced MoDL pipeline incorporating our proposed single-coil model architecture with latent transformers and learnable coil sensitivity maps.</p>
</div>
</li>
</ul>
<p class="ltx_p">Experiments were conducted on the multi-coil T1 and T2 mapping datasets. Quantitative metrics compare the two MoDL-based approaches to analyse the benefits of the proposed model enhancements, including the latent transformer’s ability to exploit inter-frame dependencies.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">The results in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Multi-coil reconstruction ‣ 3 Results ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarise the reconstruction performance for the two models across both mapping tasks. We observed consistent improvements from the proposed techniques for integrating learnable coil sensitivity estimation and advanced single-coil modelling into the MoDL framework.
</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:443.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.9pt,16.9pt) scale(0.929199892750875,0.929199892750875) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">PSNR ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">SSIM ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">NMSE ↓</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">RMSE ↓</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MoDL T1 4×</th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">34.790</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">0.894</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">0.026</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">2.85E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 8×</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">31.534</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.855</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.025</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">3.80E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 10×</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">29.904</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.840</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.030</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">4.57E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 4×</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">33.896</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.911</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.010</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">1.63E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 8×</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">29.937</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.870</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.018</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">2.54E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 10×</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">29.205</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.867</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.021</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">2.79E-05</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MoDL T1 4× + LT</th>
<td class="ltx_td ltx_align_right ltx_border_t">34.460</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.887</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.028</td>
<td class="ltx_td ltx_align_right ltx_border_t">2.97E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 8× + LT</th>
<td class="ltx_td ltx_align_right">30.763</td>
<td class="ltx_td ltx_align_right">0.837</td>
<td class="ltx_td ltx_align_right">0.028</td>
<td class="ltx_td ltx_align_right">4.15E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 10× + LT</th>
<td class="ltx_td ltx_align_right">29.762</td>
<td class="ltx_td ltx_align_right">0.839</td>
<td class="ltx_td ltx_align_right">0.032</td>
<td class="ltx_td ltx_align_right">4.67E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 4× + LT</th>
<td class="ltx_td ltx_align_right">32.866</td>
<td class="ltx_td ltx_align_right">0.899</td>
<td class="ltx_td ltx_align_right">0.011</td>
<td class="ltx_td ltx_align_right">1.83E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 8× + LT</th>
<td class="ltx_td ltx_align_right">28.853</td>
<td class="ltx_td ltx_align_right">0.847</td>
<td class="ltx_td ltx_align_right">0.023</td>
<td class="ltx_td ltx_align_right">2.89E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 10× + LT</th>
<td class="ltx_td ltx_align_right">28.005</td>
<td class="ltx_td ltx_align_right">0.844</td>
<td class="ltx_td ltx_align_right">0.028</td>
<td class="ltx_td ltx_align_right">3.22E-05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map PSNR ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map SSIM ↑</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map NMSE ↓</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt ltx_border_tt"><span class="ltx_text ltx_font_bold">Map RMSE ↓</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MoDL T1 4×</th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">22.346</span></td>
<td class="ltx_td ltx_align_right ltx_border_t">0.719</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.017</td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">105.874</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 8×</th>
<td class="ltx_td ltx_align_right">19.934</td>
<td class="ltx_td ltx_align_right">0.623</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.021</span></td>
<td class="ltx_td ltx_align_right">131.508</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 10×</th>
<td class="ltx_td ltx_align_right">20.004</td>
<td class="ltx_td ltx_align_right">0.611</td>
<td class="ltx_td ltx_align_right">0.022</td>
<td class="ltx_td ltx_align_right">129.568</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 4×</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">12.365</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.577</span></td>
<td class="ltx_td ltx_align_right">0.455</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">60.838</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 8×</th>
<td class="ltx_td ltx_align_right">11.968</td>
<td class="ltx_td ltx_align_right">0.480</td>
<td class="ltx_td ltx_align_right">0.497</td>
<td class="ltx_td ltx_align_right">63.645</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 10×</th>
<td class="ltx_td ltx_align_right">11.946</td>
<td class="ltx_td ltx_align_right">0.473</td>
<td class="ltx_td ltx_align_right">0.499</td>
<td class="ltx_td ltx_align_right">63.805</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MoDL T1 4× + LT</th>
<td class="ltx_td ltx_align_right ltx_border_t">22.035</td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">0.739</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_framed_underline">0.015</span></td>
<td class="ltx_td ltx_align_right ltx_border_t">110.417</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 8× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">20.413</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.647</span></td>
<td class="ltx_td ltx_align_right">0.023</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">124.659</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T1 10× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">20.012</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.613</span></td>
<td class="ltx_td ltx_align_right">0.022</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">129.470</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 4× + LT</th>
<td class="ltx_td ltx_align_right">12.247</td>
<td class="ltx_td ltx_align_right">0.547</td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.447</span></td>
<td class="ltx_td ltx_align_right">61.585</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MoDL T2 8× + LT</th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">12.114</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.494</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">0.481</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_framed_underline">62.617</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">MoDL T2 10× + LT</th>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">11.977</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">0.483</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">0.496</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb"><span class="ltx_text ltx_framed_underline">63.532</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Multi-coil model results for both the reconstructed MR acquisitions and the computed T1 and T2 maps. The table compares a standard MoDL model with our proposed method for artifact-removal. Our report underlined the best model for a given mapping task and acceleration factor.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">The introduction of the Latent Transformer (LT) module demonstrates clear improvements for the vast majority of single coil results across all analysed metrics, tasks, and acceleration factors, for both images and derived T1 and T2 maps (Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Single-coil reconstruction ‣ 3 Results ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). For multi-coil data, the improvements from the LT blocks are more modest. In particular, the LT addition degrades performance on reconstructed images but improves results for the computed T1 and T2 maps (Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Single-coil reconstruction ‣ 3 Results ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The difference in behaviour between the two tasks likely arises from two concurring causes. First, the network works as a regularisation term. The weighting between the data consistency layer of the CG-SENSE and network may even downgrade for heavy networks. When the network becomes too complex or contains too many parameters, it may prioritise fitting the training data over maintaining consistency with the acquired data, leading to smaller weighting and reduced effect of the LT module. This can result in degraded performance and lower accuracy in image reconstruction. From Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Multi-coil reconstruction ‣ 3 Results ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the proposed method outperform the original MoDL at higher acceleration factors, indicating the potential of the networks correcting for severe undersampling artifacts. Lighter variants on U-net with attention across channels may be taken into consideration. For the CSM update module, we also tried using the CSM generated on the fully-sampled k-space as a hard constrain to supervise, but got inferior performance. Networks with unrolled manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="A faithful deep sensitivity estimation for accelerated magnetic resonance imaging" class="ltx_ref">13</a>]</cite> or an additional J-SENSE module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="Deep j-sense: accelerated mri reconstruction via unrolled alternating optimization" class="ltx_ref">2</a>]</cite> can be incorporated to get a better data consistency performance. Second, the LT module seems to produce some image artifacts but ultimately captures inter-frame dynamics better than simpler models focused solely on de-noising, as evidenced by improved T1 and T2 map estimates. In summary, the proposed LT framework demonstrates clear utility in exploiting temporal correlations, especially for single coil acquisitions, further validating its use for reconstructing highly accelerated MRI data for T1 and T2 mapping.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">This work proposes a deep learning approach for reconstructing undersampled MRI that incorporates our novel Latent Transformers module to model inter-frame dependencies. Experiments on accelerated cardiac T1/T2 mapping show improved image quality and parameter mapping compared to baseline models, demonstrating the importance of temporal modelling. While promising, limitations remain including the basic U-Net architecture used. For future work, we will explore integrating the latent transformers into more advanced models like Restormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="Restormer: Efficient transformer for high-resolution image restoration" class="ltx_ref">15</a>]</cite> and unrolled networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="A deep cascade of convolutional neural networks for dynamic MR image reconstruction" class="ltx_ref">11</a>]</cite> with better CSM estimation to further boost performance. Overall, this study validates explicitly modelling time correlations with transformers to enable accurate reconstructions from highly accelerated quantitative MRI.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">We want to show our gratitude to Zimu Huo, who provided support and advice on multi-coil reconstruction.
This work was supported by the British Heart Foundation (RG/19/1/34160). Guang Yang was supported in part by the ERC IMI (101005122), the H2020 (952172), the MRC (MC/PC/21013), the Royal Society (IEC/NSFC/211235), the NVIDIA Academic Hardware Grant Program, the SABER project supported by Boehringer Ingelheim Ltd, and the UKRI Future Leaders Fellowship (MR/V023799/1). Fanwen Wang was supported by the UKRI Future Leaders Fellowship (MR/V023799/1). Michael Tänzer was supported by the UKRI CDT in AI for Healthcare (EP/S023283/1).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MoDL: Model-Based Deep Learning Architecture for Inverse Problems</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Medical Imaging</span> <span class="ltx_text ltx_bib_volume">38</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 394–405</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1109/TMI.2018.2865356" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p1" title="3.2 Multi-coil reconstruction ‣ 3 Results ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Arvinte, S. Vishwanath, A. H. Tewfik, and J. I. Tamir</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep j-sense: accelerated mri reconstruction via unrolled alternating optimization</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International conference on medical image computing and computer-assisted intervention</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 350–360</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Discussion ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. J. Inati, M. S. Hansen, and P. Kellman</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A fast optimal method for coil sensitivity estimation and adaptive coil combination for complex images</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 22nd Annual Meeting of ISMRM, Milan</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 4407</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.SSS3.p1" title="2.3.3 Multi-coil model ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.3.3</span></a>,
<a href="#S2.SS3.SSS3.p3" title="2.3.3 Multi-coil model ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.3.3</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Lyu, G. Li, C. Wang, C. Qin, S. Wang, Q. Dou, and J. Qin</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Region-focused multi-view transformer-based generative adversarial network for cardiac cine mri reconstruction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Medical Image Analysis</span> <span class="ltx_text ltx_bib_volume">85</span>, <span class="ltx_text ltx_bib_pages"> pp. 102760</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">O. Maier, S. H. Baete, A. Fyrdahl, K. Hammernik, S. Harrevelt, L. Kasper, A. Karakuzu, M. Loecher, F. Patzig, Y. Tian, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CG-SENSE revisited: Results from the first ISMRM reproducibility challenge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Magnetic resonance in medicine</span> <span class="ltx_text ltx_bib_volume">85</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 1821–1839</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.SSS3.p2" title="2.3.3 Multi-coil model ‣ 2.3 Model ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.3.3</span></a>.
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. R. Messroghli, K. Walters, S. Plein, P. Sparrow, M. G. Friedrich, J. P. Ridgway, and M. U. Sivananthan</span><span class="ltx_text ltx_bib_year"> (2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Myocardial T1 mapping: application to patients with acute and chronic myocardial infarction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine</span> <span class="ltx_text ltx_bib_volume">58</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 34–40</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. R. Messroghli, A. Radjenovic, S. Kozerke, D. M. Higgins, M. U. Sivananthan, and J. P. Ridgway</span><span class="ltx_text ltx_bib_year"> (2004-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Modified Look-Locker inversion recovery (MOLLI) for high-resolution T1 mapping of the heart</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Magnetic Resonance in Medicine</span> <span class="ltx_text ltx_bib_volume">52</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 141–146</span> (<span class="ltx_text ltx_bib_language">eng</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0740-3194</span>,
<a href="https://dx.doi.org/10.1002/mrm.20110" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p1" title="2.1 Data Acquisition ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. C. Moon, D. R. Messroghli, P. Kellman, S. K. Piechnik, M. D. Robson, M. Ugander, P. D. Gatehouse, A. E. Arai, M. G. Friedrich, S. Neubauer, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Myocardial T1 mapping and extracellular volume quantification: a Society for Cardiovascular Magnetic Resonance (SCMR) and CMR Working Group of the European Society of Cardiology consensus statement</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Cardiovascular Magnetic Resonance</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 1–12</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Qin, J. Duan, K. Hammernik, J. Schlemper, T. Küstner, R. Botnar, C. Prieto, A. N. Price, J. V. Hajnal, and D. Rueckert</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Complementary time-frequency domain networks for dynamic parallel MR image reconstruction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Magnetic Resonance in Medicine</span> <span class="ltx_text ltx_bib_volume">86</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 3274–3291</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. M. Sado, S. K. White, S. K. Piechnik, S. M. Banypersad, T. Treibel, G. Captur, M. Fontana, V. Maestrini, A. S. Flett, M. D. Robson, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Identification and assessment of Anderson-Fabry disease by cardiovascular magnetic resonance noncontrast myocardial T1 mapping</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Circulation: Cardiovascular Imaging</span> <span class="ltx_text ltx_bib_volume">6</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 392–398</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Schlemper, J. Caballero, J. V. Hajnal, A. N. Price, and D. Rueckert</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A deep cascade of convolutional neural networks for dynamic MR image reconstruction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE transactions on Medical Imaging</span> <span class="ltx_text ltx_bib_volume">37</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 491–503</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a href="#S5.p1" title="5 Conclusion ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Wang, J. Lyu, S. Wang, C. Qin, K. Guo, X. Zhang, X. Yu, Y. Li, F. Wang, J. Jin, Z. Shi, Z. Xu, Y. Tian, S. Hua, Z. Chen, M. Liu, M. Sun, X. Kuang, K. Wang, H. Wang, H. Li, Y. Chu, G. Yang, W. Bai, X. Zhuang, H. Wang, J. Qin, and X. Qu</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CMRxRecon: an open cardiac mri dataset for the competition of accelerated image reconstruction</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2309.10836</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p1" title="2.1 Data Acquisition ‣ 2 Materials and Methods ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Wang, H. Fang, C. Qian, B. Shi, L. Bao, L. Zhu, J. Zhou, W. Wei, J. Lin, D. Guo, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A faithful deep sensitivity estimation for accelerated magnetic resonance imaging</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2210.12723</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Discussion ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4</span></a>.
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and C. Schmid</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multiview transformers for video recognition</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3333–3343</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M. Yang</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Restormer: Efficient transformer for high-resolution image restoration</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 5728–5739</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 Conclusion ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Zhu, J. Z. Liu, S. F. Cauley, B. R. Rosen, and M. S. Rosen</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Image reconstruction by domain-transform manifold learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature</span> <span class="ltx_text ltx_bib_volume">555</span> (<span class="ltx_text ltx_bib_number">7697</span>), <span class="ltx_text ltx_bib_pages"> pp. 487–492</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ T1/T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
</footer>
</div>
</body>
</html>
